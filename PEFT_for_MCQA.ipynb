{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Cài đặt thư viện và tải bộ dữ liệu"
      ],
      "metadata": {
        "id": "SohFQ8c6kXRk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho_EvjBRjJji"
      },
      "outputs": [],
      "source": [
        "# Install libs\n",
        "!pip install-q unsloth==2025.3.15 datasets==3.5.0\n",
        "!pip install transformers bitsandbytes\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"openlifescienceai/medmcqa\")\n",
        "del ds[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modeling"
      ],
      "metadata": {
        "id": "IEwsnV18kjtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "max_seq_length = 2048\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3.2-1B-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,\n",
        "    dtype=None,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\",\n",
        "        \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
        "    use_rslora=True,\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state = 42,\n",
        "    loftq_config = None,\n",
        ")\n",
        "print(model.print_trainable_parameters())"
      ],
      "metadata": {
        "id": "gjpujt3RkRx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "h9vusllKkhV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_prompt = \"\"\"Choose the correct option for the following question.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Choice:\n",
        "{}\n",
        "\n",
        "### Answer:\n",
        "\"\"\"\n",
        "\n",
        "id2label = {\n",
        "    0: 'A',\n",
        "    1: 'B',\n",
        "    2: 'C',\n",
        "    3: 'D'\n",
        "}\n",
        "\n",
        "def formatting_prompt(examples):\n",
        "    questions = examples[\"question\"]\n",
        "    opas = examples[\"opa\"]\n",
        "    opbs = examples[\"opb\"]\n",
        "    opcs = examples[\"opc\"]\n",
        "    opds = examples[\"opd\"]\n",
        "    cops = examples[\"cop\"]\n",
        "\n",
        "    texts = []\n",
        "    for idx in range(len(questions)):\n",
        "        question = questions[idx]\n",
        "        opa = opas[idx]\n",
        "        opb = opbs[idx]\n",
        "        opc = opcs[idx]\n",
        "        opd = opds[idx]\n",
        "        answer = id2label[cops[idx]]\n",
        "        if answer == \"A\":\n",
        "            answer = answer + \" \" + opa\n",
        "        elif answer == \"B\":\n",
        "            answer = answer + \" \" + opb\n",
        "        elif answer == \"C\":\n",
        "            answer = answer + \" \" + opc\n",
        "        elif answer == \"D\":\n",
        "            answer = answer + \" \" + opd\n",
        "\n",
        "        choices = f\"A. {opa}. B. {opb}. C. {opc}. D. {opd}.\"\n",
        "        text = data_prompt.format(question, choices)\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts,}\n",
        "\n",
        "process_ds = ds.map(formatting_prompt, batched=True)"
      ],
      "metadata": {
        "id": "76DQStqEkRvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"med-mcqa-llama-3.2-1B-4bit-lora\",\n",
        "    logging_dir=\"logs\",\n",
        "    learning_rate=3e-4,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    per_device_train_batch_size=64,\n",
        "    gradient_accumulation_steps=16,\n",
        "    num_train_epochs=2,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    logging_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_steps=50,\n",
        "    logging_steps=50,\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=10,\n",
        "    seed=0,\n",
        " )\n",
        "\n",
        "trainer=SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    train_dataset=process_ds[\"train\"],\n",
        "    eval_dataset=process_ds[\"validation\"],\n",
        "    dataset_text_field=\"text\",\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "yCqIcBbDkRsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save checkpoints"
      ],
      "metadata": {
        "id": "rmCahVHxlKnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"hf_xxxxxxxxxxxxxxxx\") # your access token\n",
        "\n",
        "model.save_pretrained(\"unsloth-llama-traned\")\n",
        "PEFT_MODEL = \"your_huggingface_user_name/Llama-3.2-1B-bnb-4bit-MedMCQA\"\n",
        "model.push_to_hub(PEFT_MODEL, use_auth_token=True)"
      ],
      "metadata": {
        "id": "O02vETXHkRp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "X5jypRbwlZyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "#Option 1\n",
        "max_seq_length = 2048\n",
        "model,tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = PEFT_MODEL,\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,\n",
        "    dtype = None,\n",
        ")\n",
        "FastLanguageModel.for_inference(model)\n",
        "model.to(\"cuda\")\n",
        "\n",
        "input_text = process_ds[\"validation\"][0][\"text\"]\n",
        "\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "output = model.generate(inputs[\"input_ids\"], max_length=128)\n",
        "\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(f\"Answer: {generated_text}\")\n",
        "\n",
        "#Answer: D Local anesthesia is effective only when the nerve is not covered by myelin sheath\\n\n",
        "\n",
        "#Option 2\n",
        "generator = pipeline(\"text-generation\", model=\"thainq107/med-mcqa-llama-3.2-1B-4bit-lora\")\n",
        "\n",
        "output = generator([process_ds[\"validation\"][0][\"text\"]], max_new_tokens=128, return_full_text=False)[0]\n",
        "\n",
        "# [{\"generated_text\": \"D Local anesthesia is effective only when the nerve is not covered by myelin sheath\\n\"}]"
      ],
      "metadata": {
        "id": "SCPh-Zb5kRnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tRQblG7mkRkQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}